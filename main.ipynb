{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import bigrams\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import sweetviz as sv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import LongformerConfig, LongformerModel\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datensammlung Webseite \"Letzte Generation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_PATH = os.path.join(\"output\", \"data-lake\")\n",
    "DB_PATH = os.path.join(STORAGE_PATH, \"artikel.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapen aller Artikel auf 8 Seiten und Speichern in Sqlite-Datenbank\n",
    "# Funktion zum Scrapen der Standard-Seiten 1-7\n",
    "def scrape_standard_page(soup, c, current_date):\n",
    "    article_count = 0\n",
    "    for element in soup.find_all([\"strong\", \"span\"]):\n",
    "        if element.name == \"strong\" and element.get(\"id\", \"\").startswith(\"date_\"):\n",
    "            current_date = element.text.strip()\n",
    "        elif element.name == \"span\" and \"notranslate\" in element.get(\"class\", []):\n",
    "            source_name = (\n",
    "                element.text.split(\":\")[0].strip()\n",
    "                if \":\" in element.text\n",
    "                else element.text.strip()\n",
    "            )\n",
    "            link_tag = element.find_next(\"a\")\n",
    "            article_url = link_tag[\"href\"]\n",
    "            article_title = link_tag.text.strip()\n",
    "\n",
    "            # Überspringe JavaScript-Links\n",
    "            if \"javascript:void(0);\" in article_url:\n",
    "                continue\n",
    "\n",
    "            # Zähler erhöhen und Daten in die Datenbank einfügen\n",
    "            article_count += 1\n",
    "            c.execute(\n",
    "                \"INSERT INTO Artikel VALUES (?, ?, ?, ?)\",\n",
    "                (current_date, source_name, article_title, article_url),\n",
    "            )\n",
    "    return article_count\n",
    "\n",
    "\n",
    "# Funktion zum Scrapen der speziellen Seite (Seite 8)\n",
    "def scrape_special_page(soup, c):\n",
    "    article_count = 0\n",
    "    current_date = None\n",
    "    for element in soup.find_all([\"strong\", \"b\", \"a\"]):\n",
    "        if element.name in [\"strong\", \"b\"]:\n",
    "            date_match = re.match(r\"\\d{1,2}\\.\\d{1,2}\\.\\d{4}\", element.text.strip())\n",
    "            if date_match:\n",
    "                current_date = date_match.group()\n",
    "\n",
    "        elif element.name == \"a\" and element.has_attr(\"href\") and current_date:\n",
    "            article_url = element[\"href\"]\n",
    "            article_title = element.text.strip()\n",
    "            preceding_text = element.find_previous_sibling(string=True)\n",
    "            source_name = (\n",
    "                preceding_text.split(\":\")[0].strip()\n",
    "                if preceding_text and \":\" in preceding_text\n",
    "                else \"Unbekannt\"\n",
    "            )\n",
    "\n",
    "            # Überspringe JavaScript-Links\n",
    "            if \"javascript:void(0);\" in article_url:\n",
    "                continue\n",
    "\n",
    "            article_count += 1\n",
    "            c.execute(\n",
    "                \"INSERT INTO Artikel VALUES (?, ?, ?, ?)\",\n",
    "                (current_date, source_name, article_title, article_url),\n",
    "            )\n",
    "    return article_count\n",
    "\n",
    "\n",
    "# Hauptteil des Skripts\n",
    "conn = sqlite3.connect(os.path.join(STORAGE_PATH, \"artikel.db\"))\n",
    "c = conn.cursor()\n",
    "c.execute(\n",
    "    \"\"\"CREATE TABLE IF NOT EXISTS Artikel\n",
    "             (date text, source_name text, article_title text, article_url text)\"\"\"\n",
    ")\n",
    "\n",
    "# Ermittle die Anzahl der Seiten\n",
    "response = requests.get(\"https://letztegeneration.org/presse/berichterstattung-\")\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "pagination_links = soup.find_all(\n",
    "    \"a\", href=lambda href: href and \"berichterstattung-\" in href\n",
    ")\n",
    "highest_page_number = 0\n",
    "for link in pagination_links:\n",
    "    try:\n",
    "        page_number = int(link[\"href\"].split(\"-\")[-1])\n",
    "        highest_page_number = max(highest_page_number, page_number)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "# Scrapen aller Seiten\n",
    "for i in range(1, highest_page_number + 1):\n",
    "    url = f\"https://letztegeneration.org/presse/berichterstattung-{i}/\"\n",
    "    response = requests.get(url)\n",
    "    page_soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    current_date = None\n",
    "\n",
    "    # Überprüfe, welche Scraping-Methode anzuwenden ist\n",
    "    if page_soup.find(\"div\", class_=\"elementor-element\"):\n",
    "        article_count = scrape_special_page(page_soup, c)\n",
    "    else:\n",
    "        article_count = scrape_standard_page(page_soup, c, current_date)\n",
    "\n",
    "    print(f\"URL: {url} - Anzahl der Treffer: {article_count}\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufruf aller html-URL aus Datenbank und paralleles Scrapen der html-Dateien und Speichern der Resultate in Datenbank\n",
    "\n",
    "# Verbindung zur Datenbank herstellen\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "c = conn.cursor()\n",
    "\n",
    "# Überprüfen, ob die Spalte 'html_content' bereits existiert, und falls nicht, hinzufügen\n",
    "try:\n",
    "    c.execute(\"SELECT html_content FROM Artikel LIMIT 1\")\n",
    "except sqlite3.OperationalError:\n",
    "    c.execute(\"ALTER TABLE Artikel ADD COLUMN html_content TEXT\")\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "def fetch_html(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return url, response.text\n",
    "        else:\n",
    "            return url, None\n",
    "    except Exception as e:\n",
    "        return url, None\n",
    "\n",
    "\n",
    "def write_error_to_csv(url):\n",
    "    with open(os.path.join(\"output\", \"errors.csv\"), \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([url])\n",
    "\n",
    "\n",
    "def update_db(url, html_content):\n",
    "    try:\n",
    "        c.execute(\n",
    "            \"UPDATE Artikel SET html_content = ? WHERE article_url = ?\",\n",
    "            (html_content, url),\n",
    "        )\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        write_error_to_csv(url)\n",
    "\n",
    "\n",
    "# URLs aus der Datenbank abrufen\n",
    "c.execute(\"SELECT article_url FROM Artikel\")\n",
    "urls = c.fetchall()\n",
    "\n",
    "#Für Testzwecke: Liste auf die ersten 100 URLs begrenzen\n",
    "#urls = urls[:1000]\n",
    "\n",
    "# Paralleles Ausführen der Scraping-Funktion\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results = executor.map(fetch_html, [url[0] for url in urls])\n",
    "\n",
    "    completed = 0\n",
    "    for url, html_content in results:\n",
    "        if html_content:\n",
    "            update_db(url, html_content)\n",
    "        else:\n",
    "            write_error_to_csv(url)\n",
    "\n",
    "        completed += 1\n",
    "        if completed % 100 == 0:\n",
    "            print(f\"Bisher verarbeitete URLs: {completed}\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernen von HTML_Tags aus html_content und Anlegen neuer .csv mit | als delimiter, um es in Tableau Prep einlesen zu können\n",
    "# Funktion, um HTML-Tags zu entfernen\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Verbindung zur SQLite-Datenbank herstellen\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Laden der Daten aus der Datenbank\n",
    "df = pd.read_sql_query(\"SELECT * FROM Artikel\", conn)\n",
    "\n",
    "# Bereinigen der Daten (Beispiel: Entfernen von Nullwerten)\n",
    "df = df.dropna()\n",
    "\n",
    "# HTML-Tags aus der gewünschten Spalte entfernen\n",
    "# Anwendung von tqdm für den Fortschrittsbalken\n",
    "df['html_content'] = [remove_html_tags(x) if x else x for x in tqdm(df['html_content'], desc=\"HTML-Tags entfernen\")]\n",
    "\n",
    "# Speichern in einer neuen CSV-Datei mit alternativem Delimiter\n",
    "df.to_csv(os.path.join(\"output\", 'bereinigte_test.csv'), sep='|', index=False, encoding='utf-8')\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Datei 'bereinigte_test.csv' wurde in Tableau Prep geladen, die Daten bereinigt und normalisiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die bereinigten Daten wurden als '240102_artikel_output_cleansed.db' in eine Sqlite-Datenbank geladen. Mit dieser Datenbasis wurde weitergearbeitet (\"data/ 240102_artikel_output_cleansed.db\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCES_PATH_FINAL = os.path.join(\"data\", \"240102_artikel_output_cleansed.db\")\n",
    "STORAGE_PATH_FINAL = os.path.join(\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling report via Sweetviz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profiling report\n",
    "db_connection = sqlite3.connect(SOURCES_PATH_FINAL)\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM artikel_output_cleansed\", db_connection)\n",
    "\n",
    "report = sv.analyze(df)\n",
    "report.show_html(os.path.join(STORAGE_PATH_FINAL, 'profiling_report_letztegeneration.html'))\n",
    "\n",
    "db_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Analyse: Verteilung der Häufigkeit nach Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datenbankabfrage nach Verteilung der Häufigkeit nach Medium/source_name + Visualisierung\n",
    "db_connection = sqlite3.connect(SOURCES_PATH_FINAL)\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "# SQL-Abfrage, um die Top 15 Medien zu erhalten\n",
    "sql_top_media = \"\"\"\n",
    "    SELECT source_name, COUNT(*) AS anzahl_der_artikel\n",
    "    FROM artikel_output_cleansed\n",
    "    GROUP BY source_name\n",
    "    ORDER BY anzahl_der_artikel DESC\n",
    "    LIMIT 10;\n",
    "\"\"\"\n",
    "cursor.execute(sql_top_media)\n",
    "\n",
    "# Ergebnisse abrufen\n",
    "results = cursor.fetchall()\n",
    "\n",
    "# Daten in DataFrame konvertieren\n",
    "df = pd.DataFrame(results, columns=['Medium', 'Anzahl_der_Artikel'])\n",
    "\n",
    "# Balkendiagramm mit Seaborn erstellen\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=\"Medium\", y=\"Anzahl_der_Artikel\", data=df)\n",
    "plt.xticks(rotation=45, fontsize=8) \n",
    "ax.set_xlabel(\"Medium\", labelpad=15)\n",
    "ax.set_ylabel(\"Anzahl der Artikel\", labelpad=15)\n",
    "ax.set_title(\"TOP 10 Medien mit der höchsten Artikelanzahl\", fontsize=13)\n",
    "\n",
    "# Werte an der Spitze der Balken anzeigen\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{int(p.get_height())}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', fontsize=10, xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# Oberen und rechten Achsenlinien ausblenden\n",
    "sns.despine()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(results)\n",
    "\n",
    "db_connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Analyse: Verteilung der Häufigkeit nach Datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datenbankabfrage nach Verteilung der Häufigkeit nach Datum + Visualisierung\n",
    "db_connection = sqlite3.connect(SOURCES_PATH_FINAL)\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT date_formatted, COUNT(*) AS anzahl_der_meldungen\n",
    "    FROM sentiment\n",
    "    GROUP BY date_formatted\n",
    "    ORDER BY anzahl_der_meldungen DESC\n",
    "    LIMIT 10;\n",
    "\"\"\")\n",
    "\n",
    "# Ergebnisse in ein Pandas DataFrame laden\n",
    "df = pd.DataFrame(cursor.fetchall(), columns=[\"Datum\", \"Anzahl_der_Meldungen\"])\n",
    "\n",
    "# Diagramm erstellen, wenn Daten vorhanden sind\n",
    "if not df.empty:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=\"Datum\", y=\"Anzahl_der_Meldungen\", data=df)\n",
    "    ax.set_xlabel(\"Datum\", labelpad=10)\n",
    "    ax.set_ylabel(\"Anzahl der Meldungen\", labelpad=10)\n",
    "    ax.set_title(\"TOP 10 Tage mit der höchsten Artikelanzahl\", fontsize=13)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Werte an der Spitze der Balken anzeigen\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f\"{int(p.get_height())}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', fontsize=10, xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "    # Oberen und rechten Achsenlinien ausblenden\n",
    "    sns.despine()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Keine Ergebnisse gefunden.\")\n",
    "\n",
    "print(df)\n",
    "db_connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Analyse: Sentiment-Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristische Textbereinigungsfunktion und Sentiment-Analyse\n",
    "def clean_text_heuristically(text):\n",
    "    # Entferne Zeilenumbrüche\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    # Abschnitte mit hoher Linkdichte entfernen\n",
    "    text = re.sub(r'\\bhttps?:\\/\\/\\S+\\b', '', text)  # Entfernt URLs\n",
    "\n",
    "    # Schlüsselwörter entfernen, die auf irrelevante Abschnitte hinweisen\n",
    "    keywords_to_remove = ['Kontakt', 'Werbung', 'Impressum', 'Datenschutz']\n",
    "    for keyword in keywords_to_remove:\n",
    "        text = re.sub(r'\\b' + keyword + r'\\b', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "conn = sqlite3.connect(SOURCES_PATH_FINAL)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Neue Tabelle \"sentiment\" anlegen\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS sentiment (\n",
    "        date TEXT,\n",
    "        date_formatted TEXT,\n",
    "        source_name TEXT,\n",
    "        article_title TEXT,\n",
    "        article_url TEXT,\n",
    "        html_content TEXT,\n",
    "        sentiment_label TEXT,\n",
    "        sentiment_score REAL\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Datensätze mit minds. 20 Zeichen Länge abrufen\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT date, date_formatted, source_name, article_title, article_url, html_content \n",
    "    FROM artikel_output_cleansed \n",
    "    WHERE LENGTH(html_content) > 20\n",
    "\"\"\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Sentiment-Modell initialisieren\n",
    "pipe = pipeline(\"text-classification\", model=\"mdraw/german-news-sentiment-bert\")\n",
    "\n",
    "# Sentiment-Analyse für jeden Datensatz durchführen und Ergebnisse in der Datenbank speichern, max. Tokenlänge 512\n",
    "for row in tqdm(rows, desc=\"Analyse der Sentiments\", unit=\"text\"):\n",
    "    date, date_formatted, source_name, article_title, article_url, html_content = row\n",
    "    cleaned_text = clean_text_heuristically(html_content)  # Heuristische Textbereinigung\n",
    "    truncated_text = cleaned_text[:512] if len(cleaned_text) > 512 else cleaned_text\n",
    "    result = pipe(truncated_text)\n",
    "    sentiment_label = result[0]['label']\n",
    "    sentiment_score = result[0]['score']\n",
    "\n",
    "    # Einfügen des Datensatzes in die Tabelle \"sentiment\"\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO sentiment (date, date_formatted, source_name, article_title, article_url, html_content, sentiment_label, sentiment_score) \n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (date, date_formatted, source_name, article_title, article_url, cleaned_text, sentiment_label, sentiment_score))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Ergebnisse der Sentiment-Analyse\n",
    "\n",
    "conn = sqlite3.connect(SOURCES_PATH_FINAL)\n",
    "query = \"SELECT sentiment_score FROM sentiment\"\n",
    "df_output = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Zusammenfassende Statistiken generieren\n",
    "print(df_output[[\"sentiment_score\"]].describe())\n",
    "\n",
    "# Visualisierung der Sentiment Scores\n",
    "sns.histplot(df_output['sentiment_score'], kde=True)\n",
    "plt.title('Verteilung der Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Häufigkeit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Anzahl der sentiment label\n",
    "conn = sqlite3.connect(SOURCES_PATH_FINAL)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Anfrage, um die Anzahl der verschiedenen Labels zu ermitteln\n",
    "query = \"\"\"\n",
    "    SELECT sentiment_label, COUNT(*) \n",
    "    FROM sentiment \n",
    "    GROUP BY sentiment_label\n",
    "\"\"\"\n",
    "cursor.execute(query)\n",
    "label_counts = cursor.fetchall()\n",
    "\n",
    "# Ausgabe der Anzahl für jedes Label\n",
    "for label, count in label_counts:\n",
    "    print(f\"Label '{label}': {count} mal vorhanden\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment-Analyse der TOP 10 Tage nach Artikelanzahl\n",
    "\n",
    "# Heuristische Textbereinigungsfunktion\n",
    "def clean_text_heuristically(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'\\bhttps?:\\/\\/\\S+\\b', '', text)\n",
    "    keywords_to_remove = ['Kontakt', 'Werbung', 'Impressum', 'Datenschutz']\n",
    "    for keyword in keywords_to_remove:\n",
    "        text = re.sub(r'\\b' + keyword + r'\\b', '', text)\n",
    "    return text\n",
    "\n",
    "# Datenbankverbindung herstellen\n",
    "conn = sqlite3.connect(SOURCES_PATH_FINAL)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Sentiment-Modell initialisieren\n",
    "pipe = pipeline(\"text-classification\", model=\"mdraw/german-news-sentiment-bert\")\n",
    "\n",
    "# Datumsliste für die TOP 10 Tage\n",
    "top_10_dates = ['2023-05-25', '2023-07-13', '2022-11-24', '2022-11-09', '2022-11-04', \n",
    "                '2022-12-09', '2023-04-24', '2022-11-25', '2023-02-06', '2022-11-01']\n",
    "\n",
    "# Durchschnittliche Sentiments für die TOP 10 Tage berechnen\n",
    "for date in top_10_dates:\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT html_content \n",
    "        FROM artikel_output_cleansed \n",
    "        WHERE date_formatted = ? AND LENGTH(html_content) > 20\n",
    "    \"\"\", (date,))\n",
    "    articles = cursor.fetchall()\n",
    "\n",
    "    sentiments = []\n",
    "    for article in articles:\n",
    "        html_content = article[0]\n",
    "        cleaned_text = clean_text_heuristically(html_content)\n",
    "        truncated_text = cleaned_text[:512] if len(cleaned_text) > 512 else cleaned_text\n",
    "        sentiment_result = pipe(truncated_text)\n",
    "        sentiment_label = sentiment_result[0]['label']\n",
    "        sentiments.append(sentiment_label)\n",
    "\n",
    "    sentiment_counts = Counter(sentiments)\n",
    "    if sentiment_counts:\n",
    "        dominant_sentiment = sentiment_counts.most_common(1)[0]\n",
    "        print(f\"Datum: {date}, Dominantes Sentiment: {dominant_sentiment[0]} (Anzahl: {dominant_sentiment[1]})\")\n",
    "    else:\n",
    "        print(f\"Datum: {date}, Keine Artikel gefunden\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse der häufigsten Wörter in den Artikeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereinigen von Stopwörtern und Zahlen\n",
    "stopwords_url = \"https://raw.githubusercontent.com/solariz/german_stopwords/master/german_stopwords_full.txt\"\n",
    "stopwords_list = requests.get(stopwords_url).text.split(\"\\n\")[9:]\n",
    "# Spezifische Wörter zur Stopwörterliste hinzufügen\n",
    "additional_stopwords = [\"the\", \"generation\", \"loading\" , \"externer inhalt\", \"twitter\", \"facebook\", \"instagram\", \"impressum\", \"kontakt\", \"kleinanzeige\", \"externen inhalt\", \"news\"]\n",
    "stopwords_list.extend(additional_stopwords)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Bereinigt den Text, indem Stopwörter und kurze Wörter/Zahlen entfernt werden.\"\"\"\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    return ' '.join(word for word in words if word not in stopwords_list and len(word) > 2 and not word.isdigit())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse der häufigsten Wörter\n",
    "def count_most_common_words(db_path, table_name, column_name, num_words):\n",
    "    \"\"\" Laden der Daten, Text bereinigen und zurückgeben der häufigsten Wörter bis zu einer festgelegten Anzahl\"\"\"\n",
    "    db_connection = sqlite3.connect(db_path)\n",
    "\n",
    "    query = f\"SELECT {column_name} FROM {table_name} LIMIT 100\"\n",
    "    df = pd.read_sql_query(query, db_connection)\n",
    "\n",
    "    db_connection.close()\n",
    "\n",
    "    # Parallelisiere die Textbereinigung und Fortschrittsbalken\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        cleaned_texts = list(tqdm(executor.map(clean_text, df[column_name].dropna()), total=df[column_name].notna().sum()))\n",
    "\n",
    "    # Kombiniere alle bereinigten Texte in einer Zeichenkette\n",
    "    all_text = ' '.join(cleaned_texts)\n",
    "\n",
    "    most_common_words = Counter(all_text.split()).most_common(num_words)\n",
    "\n",
    "    return most_common_words\n",
    "\n",
    "most_common_words = count_most_common_words(SOURCES_PATH_FINAL, \"artikel_output_cleansed\", \"html_content\", 20)\n",
    "\n",
    "# Ausgabe der 20 häufigsten Worte\n",
    "for word, count in most_common_words:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling report via Sweetviz nach Sentiment-Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling report auf basis der Sentiment-Ergebnisse\n",
    "db_connection = sqlite3.connect(SOURCES_PATH_FINAL)\n",
    "df = pd.read_sql_query(\"SELECT * FROM sentiment\", db_connection)\n",
    "\n",
    "report = sv.analyze(df)\n",
    "\n",
    "report.show_html('output/profiling_report_letztegeneration_2.html')\n",
    "\n",
    "db_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Analyse: Assoziationsanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Häufige Bigramme für bestimmte Aktionsformen extrahieren\n",
    "\n",
    "def get_frequent_bigrams_for_action(cursor, action, n_most_common=5):\n",
    "    cursor.execute(\"SELECT html_content FROM sentiment WHERE html_content LIKE ?\", ('%' + action + '%',))\n",
    "    texts = [clean_text(BeautifulSoup(row[0], \"html.parser\").get_text()) for row in cursor.fetchall()]\n",
    "    all_words = list(itertools.chain.from_iterable([text.split() for text in texts]))\n",
    "    bigram_counts = Counter(bigrams(all_words))\n",
    "    return bigram_counts.most_common(n_most_common)\n",
    "\n",
    "def main():\n",
    "    conn = sqlite3.connect(SOURCES_PATH_FINAL)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    actions = [\n",
    "        \"Containern\",\n",
    "        \"Straßenblockade\",\n",
    "        \"Gebäude blockieren\",\n",
    "        \"Kunstaktion\",\n",
    "        \"Veranstaltung stören\",\n",
    "        \"gegen Reiche\"\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for action in tqdm(actions, desc=\"Verarbeite Anfrage\"):\n",
    "        common_bigrams = get_frequent_bigrams_for_action(cursor, action)\n",
    "        results.extend([(action, ' '.join(bigram), freq) for bigram, freq in common_bigrams])\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    # Ergebnisse in CSV-Datei speichern\n",
    "    with open(os.path.join(\"output\", 'associations_analysis_results.csv'), 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Aktion', 'Bigramm', 'Häufigkeit'])\n",
    "        writer.writerows(results)\n",
    "\n",
    "    #print(results)\n",
    "    return results\n",
    "\n",
    "# Die main-Funktion aufrufen und Ergebnisse in einer Variable speichern\n",
    "association_results = main()\n",
    "print(association_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Ergebnisse der Assoziationsanalyse über ein Bubble Chart\n",
    "# Erstellen eines DataFrame aus den Ergebnissen der Analyse\n",
    "df = pd.DataFrame(association_results, columns=[\"Aktion\", \"Bigramm\", \"Häufigkeit\"])\n",
    "\n",
    "# Pivot-Tabelle für die Heatmap\n",
    "pivot_table = df.pivot(\"Aktion\", \"Bigramm\", \"Häufigkeit\")\n",
    "\n",
    "# Bubble Chart erstellen\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=df, x=\"Bigramm\", y=\"Aktion\", size=\"Häufigkeit\", legend=False, sizes=(100, 2000))\n",
    "plt.title(\"Bubble Chart der Bigramme und Aktionen\", fontsize=13)\n",
    "plt.xticks(rotation=45, fontsize=8)\n",
    "sns.despine()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
